#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 28 06:20:07 2022

@authors: Claudia Donahue and Nataliya Peshekhodko

ST 590
Summer 2022
Project 3

"""
# =============================================================================
# Set up for Creating Files
# • Read in the all_accelerometer_data_pids_13.csv file
# =============================================================================




# =============================================================================
# There are lots of records per user id. Let’s just concern ourselves with person SA0297 and PC6771.
# Create two data frames, one for person SA0297’s data and one for person PC6771’s data.
# =============================================================================




# =============================================================================
# Set up a for loop to write 500 values at a time (not randomly, from first line) for SA0297 to a .csv file
# in a folder for that person’s data. Similarly output values for PC6771 to another folder. This should be
# done in the same loop. The loop should then delay for 20 seconds after writing to the files. You should
# not run the loop yet!
# =============================================================================




# =============================================================================
# Reading a Stream
# • We’re going to read in two streams (via two separate queries) for this part.
# • Setup the schema and create an input stream from the csv folder for SA0297
# =============================================================================




# =============================================================================
# Setup the schema and create another input stream from the csv folder for PC6771
# 
# =============================================================================








# =============================================================================
# Transform/Aggregation Step
# 
# Now, for each stream we’ll do a basic transformation of the x, y, and z coordinates into a magnitude.
# 
# All of the necessary functions are available from the spark.sql.functions submodule. I did this using
# the .select() method with the transformations done within .select(transform here) but there are
# multiple ways to do this.
# 
# We want to keep the time and pid columns, this new column, and drop the original x, y, and z columns.
# This was easy to incorporate within the .select() above!
# =============================================================================










# =============================================================================
# Writing the Streams
# • Lastly, we’ll write each stream out to their own csv file(s).
# – Use the csv output format
# – Use the append outputMode
# – You’ll need to include an option for checkpointlocation.
# ∗ The syntax is .option("checkpointlocation","path")
# – Lastly, start each query via the .start() method
# =============================================================================





# =============================================================================
# Now open a python console and submit the necessary code to run the loop that outputs data.
# Let this all run for about 5 minutes. Stop both queries using the .stop() method.
# =============================================================================




# =============================================================================
# Now What?
# • The output is in .csv files for each user but not very easy to deal with because it is split up
# • Using code from 
# [here](https://community.cloudera.com/t5/Support-Questions/Structured-Streaming-writestream-append-to-file/m-p/161638), 
# we can read in all the pieces and output each to their own single .csv file.
# =============================================================================


# Use PySpark to read in all "part" files
allfiles = spark.read.option("header","false").csv("/destination_path/part-*.csv")
# Output as CSV file
allfiles \
.coalesce(1) \
.write.format("csv") \
.option("header", "false") \
.save("/destination_path/single_csv_file/")














